# Host Configuration
host_ip=localhost

# Proxy Configuration (if needed)
no_proxy=
http_proxy=
https_proxy=

# LLM Service Configuration
LLM_ENDPOINT_PORT=8008
LLM_MODEL_ID=llama3.2:1b

# Guardrails Service Configuration
GUARDRAILS_PORT=9090  # Updated port to match service
HUGGING_FACE_TOKEN=  # Replace with your actual token
HUGGING_FACE_HUB_TOKEN=
HF_TOKEN=

# TGI Server Configuration
TGI_PORT=80
SAFETY_GUARD_MODEL_ID="meta-llama/Meta-Llama-Guard-2-8B"


GUARDRAILS_COMPONENT_NAME=OPEA_LLAMA_GUARD
SAFETY_GUARD_ENDPOINT=http://${host_ip}:${TGI_PORT}
MAX_INPUT_TOKENS=2048
MAX_TOTAL_TOKENS=4096
DATA_PATH=./data

OLLAMA_PORT=11434


export HUGGING_FACE_TOKEN=
